{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fa3cea2",
   "metadata": {},
   "source": [
    "# multi-label stratification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8958fadd",
   "metadata": {},
   "source": [
    "## here we do train/dev splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc26e9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Original Data Info ---\n",
      "Total number of patients before filtering: 720\n",
      "\n",
      "Original Diagnosis Distribution:\n",
      "clinical_diagnosis\n",
      "AD       405\n",
      "CN       229\n",
      "FTD       53\n",
      "AFM       17\n",
      "Other     16\n",
      "Name: count, dtype: int64\n",
      "\n",
      "==================================================\n",
      "\n",
      "--- Data After Filtering for ['CN', 'AD', 'FTD'] ---\n",
      "Total number of patients for splitting: 687\n",
      "\n",
      "Filtered Diagnosis Distribution:\n",
      "clinical_diagnosis\n",
      "AD     0.589520\n",
      "CN     0.333333\n",
      "FTD    0.077147\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Filtered Site Distribution:\n",
      "site\n",
      "Lopera    1.0\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "==================================================\n",
      "\n",
      "--- Combination Key Counts ---\n",
      "stratify_key\n",
      "AD_Lopera     405\n",
      "CN_Lopera     229\n",
      "FTD_Lopera     53\n",
      "Name: count, dtype: int64\n",
      "\n",
      "==================================================\n",
      "\n",
      "--- Verification of Splits ---\n",
      "Total patients in sets: 687 (should match filtered total)\n",
      "Training patients:   411\n",
      "Validation patients: 138\n",
      "Test patients:       138\n",
      "\n",
      "--- Diagnosis Distribution ---\n",
      "Train Set Diagnosis Distribution:\n",
      "clinical_diagnosis\n",
      "AD     0.591241\n",
      "CN     0.333333\n",
      "FTD    0.075426\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Validation Set Diagnosis Distribution:\n",
      "clinical_diagnosis\n",
      "AD     0.586957\n",
      "CN     0.333333\n",
      "FTD    0.079710\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Test Set Diagnosis Distribution:\n",
      "clinical_diagnosis\n",
      "AD     0.586957\n",
      "CN     0.333333\n",
      "FTD    0.079710\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "==================================================\n",
      "\n",
      "--- Site Distribution ---\n",
      "Train Set Site Distribution:\n",
      "site\n",
      "Lopera    1.0\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Validation Set Site Distribution:\n",
      "site\n",
      "Lopera    1.0\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Test Set Site Distribution:\n",
      "site\n",
      "Lopera    1.0\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "==================================================\n",
      "\n",
      "✅ Training metadata saved to: ../data/train_metadata.csv\n",
      "✅ Validation metadata saved to: ../data/validation_metadata.csv\n",
      "✅ Test metadata saved to: ../data/test_metadata.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- 1. Load and Prepare Data ---\n",
    "\n",
    "metadata_df = pd.read_csv('../metadata/metadata.csvz')\n",
    "\n",
    "print(\"--- Original Data Info ---\")\n",
    "print(f\"Total number of patients: {len(metadata_df)}\")\n",
    "print(\"\\nOriginal Diagnosis Distribution:\")\n",
    "print(metadata_df['clinical_diagnosis'].value_counts(normalize=True))\n",
    "print(\"\\nOriginal Site Distribution:\")\n",
    "print(metadata_df['site'].value_counts(normalize=True))\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "\n",
    "# --- 2. Create the Combination Key for Stratification ---\n",
    "metadata_df['stratify_key'] = metadata_df['clinical_diagnosis'] + '_' + metadata_df['site']\n",
    "\n",
    "print(\"--- Combination Key Counts ---\")\n",
    "print(metadata_df['stratify_key'].value_counts())\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "\n",
    "# --- 3. CRITICAL: Check for Rare Combinations ---\n",
    "# Stratification requires at least 2 samples for each class. If any combination\n",
    "# has only 1 sample, the split will fail.\n",
    "key_counts = metadata_df['stratify_key'].value_counts()\n",
    "if (key_counts < 2).any():\n",
    "    print(\"!!! WARNING: Found combinations with only 1 sample. !!!\")\n",
    "    print(\"The following combinations are too rare for stratification:\")\n",
    "    print(key_counts[key_counts < 2])\n",
    "    print(\"\\nYou must handle this before proceeding. Options:\")\n",
    "    print(\"1. Get more data for these groups.\")\n",
    "    print(\"2. Group rare sites together (e.g., 'SiteC' and 'SiteD' become 'Other_Site').\")\n",
    "    print(\"3. Remove these single-sample patients (last resort).\")\n",
    "    # Exit or handle the issue here. For this script, we'll assume no error.\n",
    "    # exit()\n",
    "\n",
    "\n",
    "# --- 4. Perform the Stratified Splits using the Combination Key ---\n",
    "\n",
    "# Define split proportions\n",
    "TEST_SIZE = 0.20      # 20% of patients for the test set\n",
    "VALIDATION_SIZE = 0.25 # 25% of the remaining 80% for validation (i.e., 20% of total)\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# First split: (Train + Validation) vs. Test\n",
    "# We split the DataFrame directly since each row is a patient\n",
    "train_val_df, test_df = train_test_split(\n",
    "    metadata_df,\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=metadata_df['stratify_key'] # Stratify on our new key\n",
    ")\n",
    "\n",
    "# Second split: Train vs. Validation\n",
    "train_df, val_df = train_test_split(\n",
    "    train_val_df,\n",
    "    test_size=VALIDATION_SIZE,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=train_val_df['stratify_key'] # Stratify again on the same key\n",
    ")\n",
    "\n",
    "# Clean up the temporary key from the final dataframes\n",
    "train_df = train_df.drop(columns=['stratify_key'])\n",
    "val_df = val_df.drop(columns=['stratify_key'])\n",
    "test_df = test_df.drop(columns=['stratify_key'])\n",
    "\n",
    "\n",
    "# --- 5. Verification ---\n",
    "print(\"--- Verification of Splits ---\")\n",
    "print(f\"Total patients in sets: {len(train_df) + len(val_df) + len(test_df)} (should match total)\")\n",
    "print(f\"Training patients:   {len(train_df)}\")\n",
    "print(f\"Validation patients: {len(val_df)}\")\n",
    "print(f\"Test patients:       {len(test_df)}\\n\")\n",
    "\n",
    "print(\"--- Diagnosis Distribution ---\")\n",
    "print(\"Train Set Diagnosis Distribution:\")\n",
    "print(train_df['clinical_diagnosis'].value_counts(normalize=True).sort_index())\n",
    "print(\"\\nValidation Set Diagnosis Distribution:\")\n",
    "print(val_df['clinical_diagnosis'].value_counts(normalize=True).sort_index())\n",
    "print(\"\\nTest Set Diagnosis Distribution:\")\n",
    "print(test_df['clinical_diagnosis'].value_counts(normalize=True).sort_index())\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "\n",
    "print(\"--- Site Distribution ---\")\n",
    "print(\"Train Set Site Distribution:\")\n",
    "print(train_df['site'].value_counts(normalize=True).sort_index())\n",
    "print(\"\\nValidation Set Site Distribution:\")\n",
    "print(val_df['site'].value_counts(normalize=True).sort_index())\n",
    "print(\"\\nTest Set Site Distribution:\")\n",
    "print(test_df['site'].value_counts(normalize=True).sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53cd178",
   "metadata": {},
   "source": [
    "## Analysis of Your Results\n",
    "1. Successful Combination and Stratification:\n",
    "\n",
    "Combination Key Counts: You can see the breakdown of every diagnosis-site pair. Crucially, the smallest counts are AD_Behrens and FTD_Behrens with 2 samples each. Because no group had a count of 1, the train_test_split function was able to perform the stratification without errors. This is a perfect outcome.\n",
    "\n",
    "2. Perfect Split Proportions:\n",
    "\n",
    "#### Patient Counts:\n",
    "You started with 714 patients and ended with:\n",
    "Training: 428 patients (~60%)\n",
    "Validation: 143 patients (~20%)\n",
    "Test: 143 patients (~20%)\n",
    "Total: 428 + 143 + 143 = 714. The math is correct.\n",
    "\n",
    "3. Outstanding Distribution Balance (The Key Success Metric):\n",
    "\n",
    "#### Diagnosis Distribution: \n",
    "Look how closely the proportions match across the sets:\n",
    "\n",
    "Original: AD (33.3%), CN (33.3%), FTD (33.3%)\n",
    "Train: AD (33.1%), CN (33.1%), FTD (33.6%)\n",
    "Validation: AD (33.5%), CN (34.2%), FTD (32.1%)\n",
    "Test: AD (33.5%), CN (32.8%), FTD (33.5%)\n",
    "This is an almost perfect stratification. The very minor differences are due to the rounding required when dealing with a finite number of patients.\n",
    "Site Distribution: This is equally impressive. The proportions of patients from each site are very consistent across the three sets. For example:\n",
    "\n",
    "Custodio: ~33.4% in all three sets.\n",
    "Lopera: ~21.5% in all three sets.\n",
    "Even the smaller sites like Behrens and Bruno are represented proportionally (any small variations are expected due to the low absolute numbers).\n",
    "\n",
    "## Conclusion: \n",
    "You have successfully created high-quality, balanced, and non-overlapping training, validation, and test sets. You've accounted for the confounding variables of clinical_diagnosis and site, which gives your future models the best possible chance of learning true biological signals instead of dataset biases. This is a critical milestone passed with flying colors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83ec558",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf69b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrames saved successfully:\n",
      "- Training data: ../metadata/train_metadata.csv\n",
      "- Validation data: ../metadata/validation_metadata.csv\n",
      "- Test data: ../metadata/test_metadata.csv\n"
     ]
    }
   ],
   "source": [
    "# --- Define filenames ---\n",
    "train_output_path = '../metadata/train_metadata.csv'\n",
    "val_output_path = '../metadata/validation_metadata.csv'\n",
    "test_output_path = '../metadata/test_metadata.csv'\n",
    "\n",
    "# --- Save DataFrames to CSV ---\n",
    "# Using index=False is important to prevent pandas from writing the DataFrame index\n",
    "# as an extra column in your CSV files.\n",
    "train_df.to_csv(train_output_path, index=False)\n",
    "val_df.to_csv(val_output_path, index=False)\n",
    "test_df.to_csv(test_output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975d44f5",
   "metadata": {},
   "source": [
    "# LOPERA SUBSET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68ce636",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- 1. Load Full Data ---\n",
    "# Load the original, complete metadata file\n",
    "metadata_df_full = pd.read_csv('../data/metadata_24-09-25_lopera.csv')\n",
    "\n",
    "print(\"--- Original Data Info ---\")\n",
    "print(f\"Total number of patients before filtering: {len(metadata_df_full)}\")\n",
    "print(\"\\nOriginal Diagnosis Distribution:\")\n",
    "print(metadata_df_full['clinical_diagnosis'].value_counts())\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "\n",
    "# --- 2. Filter for Specific Diagnoses ---\n",
    "# Define the diagnoses you want to keep\n",
    "classes_to_keep = ['CN', 'AD', 'FTD']\n",
    "\n",
    "# Create a new DataFrame containing only the rows with the desired diagnoses.\n",
    "# .copy() is used to prevent a pandas SettingWithCopyWarning later.\n",
    "metadata_df = metadata_df_full[metadata_df_full['clinical_diagnosis'].isin(classes_to_keep)].copy()\n",
    "\n",
    "print(f\"--- Data After Filtering for {classes_to_keep} ---\")\n",
    "print(f\"Total number of patients for splitting: {len(metadata_df)}\")\n",
    "print(\"\\nFiltered Diagnosis Distribution:\")\n",
    "print(metadata_df['clinical_diagnosis'].value_counts(normalize=True))\n",
    "print(\"\\nFiltered Site Distribution:\")\n",
    "print(metadata_df['site'].value_counts(normalize=True))\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "\n",
    "# --- 3. Create the Combination Key for Stratification ---\n",
    "# This step and all subsequent steps now use the filtered `metadata_df`\n",
    "metadata_df['stratify_key'] = metadata_df['clinical_diagnosis'] + '_' + metadata_df['site']\n",
    "\n",
    "print(\"--- Combination Key Counts ---\")\n",
    "print(metadata_df['stratify_key'].value_counts())\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "\n",
    "# --- 4. CRITICAL: Check for Rare Combinations ---\n",
    "# Stratification requires at least 2 samples for each class.\n",
    "key_counts = metadata_df['stratify_key'].value_counts()\n",
    "if (key_counts < 2).any():\n",
    "    print(\"!!! WARNING: Found combinations with only 1 sample. !!!\")\n",
    "    print(\"The following combinations are too rare for stratification:\")\n",
    "    print(key_counts[key_counts < 2])\n",
    "    print(\"\\nYou must handle this before proceeding. Options:\")\n",
    "    print(\"1. Get more data for these groups.\")\n",
    "    print(\"2. Group rare sites together (e.g., 'SiteC' and 'SiteD' become 'Other_Site').\")\n",
    "    print(\"3. Remove these single-sample patients (last resort).\")\n",
    "    # For this script, we'll assume no error and proceed.\n",
    "    # exit()\n",
    "\n",
    "\n",
    "# --- 5. Perform the Stratified Splits using the Combination Key ---\n",
    "\n",
    "# Define split proportions\n",
    "TEST_SIZE = 0.20      # 20% of patients for the test set\n",
    "VALIDATION_SIZE = 0.25 # 25% of the remaining 80% for validation (i.e., 20% of total)\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# First split: (Train + Validation) vs. Test\n",
    "train_val_df, test_df = train_test_split(\n",
    "    metadata_df,\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=metadata_df['stratify_key']\n",
    ")\n",
    "\n",
    "# Second split: Train vs. Validation\n",
    "train_df, val_df = train_test_split(\n",
    "    train_val_df,\n",
    "    test_size=VALIDATION_SIZE,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=train_val_df['stratify_key']\n",
    ")\n",
    "\n",
    "# Clean up the temporary key from the final dataframes\n",
    "train_df = train_df.drop(columns=['stratify_key'])\n",
    "val_df = val_df.drop(columns=['stratify_key'])\n",
    "test_df = test_df.drop(columns=['stratify_key'])\n",
    "\n",
    "\n",
    "# --- 6. Verification ---\n",
    "print(\"--- Verification of Splits ---\")\n",
    "print(f\"Total patients in sets: {len(train_df) + len(val_df) + len(test_df)} (should match filtered total)\")\n",
    "print(f\"Training patients:   {len(train_df)}\")\n",
    "print(f\"Validation patients: {len(val_df)}\")\n",
    "print(f\"Test patients:       {len(test_df)}\\n\")\n",
    "\n",
    "print(\"--- Diagnosis Distribution ---\")\n",
    "print(\"Train Set Diagnosis Distribution:\")\n",
    "print(train_df['clinical_diagnosis'].value_counts(normalize=True).sort_index())\n",
    "print(\"\\nValidation Set Diagnosis Distribution:\")\n",
    "print(val_df['clinical_diagnosis'].value_counts(normalize=True).sort_index())\n",
    "print(\"\\nTest Set Diagnosis Distribution:\")\n",
    "print(test_df['clinical_diagnosis'].value_counts(normalize=True).sort_index())\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "\n",
    "print(\"--- Site Distribution ---\")\n",
    "print(\"Train Set Site Distribution:\")\n",
    "print(train_df['site'].value_counts(normalize=True).sort_index())\n",
    "print(\"\\nValidation Set Site Distribution:\")\n",
    "print(val_df['site'].value_counts(normalize=True).sort_index())\n",
    "print(\"\\nTest Set Site Distribution:\")\n",
    "print(test_df['site'].value_counts(normalize=True).sort_index())\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# --- 7. Save the final CSVs ---\n",
    "# Define output file paths\n",
    "TRAIN_PATH = '../data/train_metadata.csv'\n",
    "VAL_PATH = '../data/validation_metadata.csv'\n",
    "TEST_PATH = '../data/test_metadata.csv'\n",
    "\n",
    "# Save the dataframes to CSV\n",
    "train_df.to_csv(TRAIN_PATH, index=False)\n",
    "val_df.to_csv(VAL_PATH, index=False)\n",
    "test_df.to_csv(TEST_PATH, index=False)\n",
    "\n",
    "print(f\"✅ Training metadata saved to: {TRAIN_PATH}\")\n",
    "print(f\"✅ Validation metadata saved to: {VAL_PATH}\")\n",
    "print(f\"✅ Test metadata saved to: {TEST_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio_emb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
